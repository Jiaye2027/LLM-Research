{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"It is a starter.\n\nNext step: \n\nTest the code. Compare KG_as_RAG LLM with a LLM chat model without KG, take the difference.\n\nGeneralized the subject (not only for probability, but based on the send in user_input/KG/LLM_extraction)\n\nConsider removing the use of KG memory and session id (not necessary, and increase computational cost in the current code. However, these are necessary if it is a conversation, or include follow-up questions, which is common in probability tests)\n\nResearch the most recent RAG approaches","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain.tools import Tool\nfrom langchain import hub\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain.schema import StrOutputParser\nfrom langchain_community.chat_message_histories import Neo4jChatMessageHistory\nfrom langchain_community.graphs import Neo4jGraph\nfrom langchain_community.vectorstores import Neo4jVector\nfrom uuid import uuid4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SETUPs\n# Add the path of the file (excel file containing quetions for accuracy test)\npath = ''\nquestions_col_name = ''\ntrue_answers_col_name = ''\n\n# Set up API key\nOPENAI_API_KEY = \"sk-...\"\n\n# Connect to the Neo4j graph database (my current neo4j instance)\ngraph = Neo4jGraph(\n    url=\"neo4j+s://1e69f7f9.databases.neo4j.io\",\n    username=\"neo4j\",\n    password=\"0qkjXy8Xu3IV0bvwNKR_uPkunv7SbZf92X28jJuLiBY\"\n)\n\n# Chat prompt that only return answer, so can directly calculate accuracy\nchat_prompt_answer = \"You are a probability expert. You answer questions about probability concepts, methods, applications, and calculations. You should only return the answer. For example, If the answer is A, then you just output A.\"\n# Chat prompt for cheat sheet\nchat_prompt_cheatsheet = \"You are a probability expert. You summarize the important concepts, methods, formulas in the provided text.\n\n# Load the agent prompt from LagnSmith\nagent_prompt = hub.pull(\"hwchase17/react-chat\") # a common agent, should consider create one for this specific task","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_excel(path)\ndf.to_csv(\"csv_file.csv\", index=False)\nquestions = df[questions_col_name]\ntrue_answers = df[true_answers_col_name]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine LLM and KG, use KG as RAG\ndef KG_as_RAG(OPENAI_API_KEY = OPENAI_API_KEY, graph = graph, chat_prompt = chat_prompt_answer, agent_prompt = agent_prompt):\n    # Unique session ID\n    SESSION_ID = str(uuid4())\n    print(f\"Session ID: {SESSION_ID}\")\n    \n    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n    embedding_provider = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n    \n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                chat_prompt,\n            ),\n            (\"human\", \"{input}\"),\n        ]\n    )\n    \n    prob_chat = prompt | llm | StrOutputParser()\n    \n    # Initialize a vector retriever using Neo4jVector for semantic search\n    prob_vector = Neo4jVector.from_existing_index(\n        embedding_provider,\n        graph=graph,\n        index_name=\"probabilityConcepts\",\n        embedding_node_property=\"conceptEmbedding\",\n        text_node_property=\"conceptDescription\",\n    )\n    \n    prob_retriever = RetrievalQA.from_llm(\n        llm=llm,\n        retriever=prob_vector.as_retriever()\n    )\n    \n    # Using KG as RAG\n    def combined_prob_tool(input_text):\n        retrieved_knowledge = prob_retriever.invoke(input_text)\n        combined_input = f\"User Query: {input_text}\\n\\nRelevant Knowledge: {retrieved_knowledge}\"\n        response = prob_chat.invoke(combined_input)\n        return response\n    \n    # Memory\n    def get_memory(session_id):\n        return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n    \n    tools = [\n        Tool.from_function(\n            name=\"Probability Expert\",\n            description=(\n                \"Use this tool to answer probability-related questions. It first searches for relevant knowledge in the knowledge graph, then provides a response using the retrieved information and the user's query.\"\n            ),\n            func=combined_prob_tool,\n        ),\n    ]\n    \n    agent = create_react_agent(llm, tools, agent_prompt)\n    agent_executor = AgentExecutor(agent=agent, tools=tools)\n    \n    chat_agent = RunnableWithMessageHistory(\n        agent_executor,\n        get_memory,\n        input_messages_key=\"input\",\n        history_messages_key=\"chat_history\",\n    )\n    \n    return chat_agent","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chat_agent = KG_as_RAG()\n\n# Iterate over each question, get response, and store results\nresponses = []\nfor question in df[questions_col_name]:\n    response = chat_agent.invoke(\n        {\"input\": question},\n        {\"configurable\": {\"session_id\": str(uuid4())}},\n    )\n    responses.append(response[\"output\"])\n\ndf['responses'] = responses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate accuracy\ndf['is_correct'] = df['responses'] == df['true_answers']\naccuracy = df['is_correct'].mean()\n\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
