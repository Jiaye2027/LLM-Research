{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"},{"sourceId":208456038,"sourceType":"kernelVersion"},{"sourceId":236869,"sourceType":"modelInstanceVersion","modelInstanceId":202302,"modelId":224053},{"sourceId":238914,"sourceType":"modelInstanceVersion","modelInstanceId":204046,"modelId":225262},{"sourceId":238917,"sourceType":"modelInstanceVersion","modelInstanceId":204048,"modelId":225262},{"sourceId":238929,"sourceType":"modelInstanceVersion","modelInstanceId":204059,"modelId":225262}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install langchain_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T02:55:15.285794Z","iopub.execute_input":"2025-02-08T02:55:15.286118Z","iopub.status.idle":"2025-02-08T02:55:43.126501Z","shell.execute_reply.started":"2025-02-08T02:55:15.286096Z","shell.execute_reply":"2025-02-08T02:55:43.125488Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting langchain_community\n  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain-core<1.0.0,>=0.3.34 (from langchain_community)\n  Downloading langchain_core-0.3.34-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.18 (from langchain_community)\n  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain_community)\n  Downloading SQLAlchemy-2.0.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting requests<3,>=2 (from langchain_community)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting PyYAML>=5.3 (from langchain_community)\n  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\n  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\nCollecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting langsmith<0.4,>=0.1.125 (from langchain_community)\n  Downloading langsmith-0.3.7-py3-none-any.whl.metadata (14 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting numpy<2,>=1.26.4 (from langchain_community)\n  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\nCollecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.18->langchain_community)\n  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain<1.0.0,>=0.3.18->langchain_community)\n  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.34->langchain_community)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.34->langchain_community)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.34->langchain_community)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nCollecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nCollecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain_community)\n  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\nCollecting idna<4,>=2.5 (from requests<3,>=2->langchain_community)\n  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\nCollecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain_community)\n  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting certifi>=2017.4.17 (from requests<3,>=2->langchain_community)\n  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\nCollecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\n  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nCollecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_community)\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\nCollecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community)\n  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\nCollecting exceptiongroup>=1.0.2 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\nCollecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community)\n  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\nDownloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.34-py3-none-any.whl (412 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.3.7-py3-none-any.whl (332 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\nDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-2.0.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\nDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\nDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading attrs-25.1.0-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\nDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\nDownloading anyio-4.8.0-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\nDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\nInstalling collected packages: zstandard, urllib3, typing-extensions, tenacity, sniffio, PyYAML, python-dotenv, propcache, packaging, orjson, numpy, mypy-extensions, jsonpointer, idna, httpx-sse, h11, greenlet, frozenlist, exceptiongroup, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, typing-inspect, SQLAlchemy, requests, pydantic-core, multidict, marshmallow, jsonpatch, httpcore, anyio, aiosignal, yarl, requests-toolbelt, pydantic, httpx, dataclasses-json, pydantic-settings, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain_community\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.1.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.0 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.28.3 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.38 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 async-timeout-4.0.3 attrs-25.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 dataclasses-json-0.6.7 exceptiongroup-1.2.2 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.18 langchain-core-0.3.34 langchain-text-splitters-0.3.6 langchain_community-0.3.17 langsmith-0.3.7 marshmallow-3.26.1 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.15 packaging-24.2 propcache-0.2.1 pydantic-2.10.6 pydantic-core-2.27.2 pydantic-settings-2.7.1 python-dotenv-1.0.1 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 typing-extensions-4.12.2 typing-inspect-0.9.0 urllib3-2.3.0 yarl-1.18.3 zstandard-0.23.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# runnable\nimport torch\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Set model path\nmodel_path = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Create a Hugging Face pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=2084,\n    temperature=0.7,\n    device_map=\"auto\"\n)\n\n# Wrap the pipeline in LangChain's HuggingFacePipeline\nllm = HuggingFacePipeline(pipeline=pipe)\n\n# Define the system prompt for math and AI/ML/DL\nsystem_prompt = \"\"\"You are an expert in mathematics and AI/ML/DL. You assist with:\n1. Mathematical concepts, proofs, and problem-solving.\n2. AI/ML/DL theory, algorithms, and applications.\n3. Code implementation for AI/ML/DL projects.\n4. Debugging and optimizing AI/ML/DL workflows.\n\nProvide clear, concise, and accurate responses. If the question is unclear, ask for clarification.\"\"\"\n\n# Set up the chat prompt template\nprompt_template = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_prompt),\n    HumanMessagePromptTemplate.from_template(\"{input}\")\n])\n\n# Use latest LangChain API (RunnableSequence)\nchat_chain = prompt_template | llm\n\n# Chat loop\ndef start_chat():\n    print(\"Math & AI/ML/DL Assistant - Type 'exit' to quit\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n        response = chat_chain.invoke({\"input\": user_input})\n        print(f\"AI: {response}\")\n\n# Start chat session\nstart_chat()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install --target=/kaggle/working langchain_community\nimport os\nimport torch\nfrom langchain_community.llms import VLLM\nfrom langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom vllm import SamplingParams\n\n\n# Set environment variables (optional but recommended)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Adjust based on available GPUs\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid tokenizer parallelism conflicts\n\n\nmodel_path = \"/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1\"\n\n\n# Sampling Parameters for vLLM\nsampling_params = SamplingParams(\n    temperature=1.0,              # Control randomness\n    min_p=0.01,                   # Minimum cumulative probability for nucleus sampling\n    skip_special_tokens=True,      # Remove special tokens from output\n    max_tokens=8192,               # Maximum output length\n)\n\n# ✅ Use LangChain’s VLLM wrapper\nllm = VLLM(\n    model=model_path,\n    dtype=\"half\",                 # Use FP16 for efficiency\n    tensor_parallel_size=2,        # Use multiple GPUs (adjust as needed)\n    trust_remote_code=True,        # Trust remote execution for model and tokenizer\n    max_num_seqs=16,               # Maximum batch size per iteration\n    max_model_len=8192,            # Context length\n    gpu_memory_utilization=0.95,   # Use 95% of GPU memory\n    sampling_params=sampling_params # Pass the sampling params\n)\n\n# Define system prompt for AI/ML/DL assistance\nsystem_prompt = \"\"\"You are an expert in mathematics and AI/ML/DL. You assist with:\n1. Mathematical concepts, proofs, and problem-solving.\n2. AI/ML/DL theory, algorithms, and applications.\n3. Code implementation for AI/ML/DL projects.\n4. Debugging and optimizing AI/ML/DL workflows.\n\nProvide clear, concise, and accurate responses. If the question is unclear, ask for clarification.\"\"\"\n\n# Define chat prompt template\nprompt_template = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_prompt),\n    HumanMessagePromptTemplate.from_template(\"{input}\")\n])\n\n# ✅ Create LangChain Chat Pipeline with vLLM\nchat_chain = prompt_template | llm\n\n# Interactive Chat Loop\ndef start_chat():\n    print(\"Math & AI/ML/DL Assistant - Type 'exit' to quit\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n        response = chat_chain.invoke({\"input\": user_input})\n        print(f\"AI: {response}\")\n\n# Start Chat\nstart_chat()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T02:55:43.127794Z","iopub.execute_input":"2025-02-08T02:55:43.128055Z","iopub.status.idle":"2025-02-08T02:56:35.347908Z","shell.execute_reply.started":"2025-02-08T02:55:43.128032Z","shell.execute_reply":"2025-02-08T02:56:35.346036Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f77cfaedcf14>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# ✅ Use LangChain’s VLLM wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m llm = VLLM(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"half\"\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# Use FP16 for efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             warnings.warn(\n","\u001b[0;31mValidationError\u001b[0m: 1 validation error for VLLM\n  Value error, Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1'. Use `repo_type` argument if needed. [type=value_error, input_value={'model': '/kaggle/input/...gs': {}, 'client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"],"ename":"ValidationError","evalue":"1 validation error for VLLM\n  Value error, Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1'. Use `repo_type` argument if needed. [type=value_error, input_value={'model': '/kaggle/input/...gs': {}, 'client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nimport os\n\n# Set up environment\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Ensure GPUs are visible\n\n# Define model path (local)\nmodel_path = \"/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1\"\n\n# Initialize vLLM model\nllm = LLM(\n    model=model_path,  # ✅ Use vLLM directly (NOT LangChain's VLLM)\n    dtype=\"half\",\n    tensor_parallel_size=2,  # Use multiple GPUs\n    trust_remote_code=True,\n)\n\n# Define sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=1024\n)\n\n# Generate text using vLLM\noutputs = llm.generate([\"Explain LangChain in simple terms\"], sampling_params)\nprint(outputs[0].outputs[0].text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T02:56:35.348464Z","iopub.status.idle":"2025-02-08T02:56:35.348732Z","shell.execute_reply":"2025-02-08T02:56:35.348626Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom vllm import LLM, SamplingParams\nfrom langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom langchain_core.language_models.llms import LLM as LangChainLLM\nimport os\nimport warnings\n\nwarnings.simplefilter('ignore')\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# Set model path\nmodel_path = \"/kaggle/input/m/huikang/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1\"\n\nMAX_NUM_SEQS = 16\nMAX_MODEL_LEN = 8192\nprint('LLM')\nllm = LLM(\n    model_path,\n    dtype=\"half\",                # The data type for the model weights and activations\n    max_num_seqs=MAX_NUM_SEQS,   # Maximum number of sequences per iteration. Default is 256\n    max_model_len=MAX_MODEL_LEN, # Model context length\n    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n    tensor_parallel_size=2,      # The number of GPUs to use for distributed execution with tensor parallelism\n    gpu_memory_utilization=0.95, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n    seed=2024,\n)\nsampling_params = SamplingParams(\n    temperature=1.0,              # randomness of the sampling\n    min_p=0.01,\n    skip_special_tokens=True,     # Whether to skip special tokens in the output\n    max_tokens=MAX_MODEL_LEN,\n)\n\n# Custom LangChain wrapper for vLLM\nclass CustomVLLM(LangChainLLM):\n    def _call(self, prompt: str, stop=None) -> str:\n        \"\"\"Generate response using vLLM.\"\"\"\n        outputs = llm.generate([prompt], sampling_params)\n        return outputs[0].outputs[0].text.strip()  # Extract generated text\n\n    @property\n    def _llm_type(self) -> str:\n        return \"vllm\"\n\n# Instantiate LangChain LLM\nllm = CustomVLLM()\n\n# Define the system prompt for math and AI/ML/DL\nsystem_prompt = \"\"\"You are an expert in mathematics and AI/ML/DL. You assist with:\n1. Mathematical concepts, proofs, and problem-solving.\n2. AI/ML/DL theory, algorithms, and applications.\n3. Code implementation for AI/ML/DL projects.\n4. Debugging and optimizing AI/ML/DL workflows.\n\nProvide clear, concise, and accurate responses. If the question is unclear, ask for clarification.\"\"\"\n\n# Set up the chat prompt template\nprompt_template = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_prompt),\n    HumanMessagePromptTemplate.from_template(\"{input}\")\n])\n\n# Use latest LangChain API (RunnableSequence)\nchat_chain = prompt_template | llm\n\n# Chat loop\ndef start_chat():\n    print(\"Math & AI/ML/DL Assistant - Type 'exit' to quit\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n        response = chat_chain.invoke({\"input\": user_input})\n        print(f\"AI: {response}\")\n\n# Start chat session\nstart_chat()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_chat()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom vllm import LLM, SamplingParams\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_community.llms import VLLM\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\" # \"0,1,2,3\"\n\n# Set model path\nmodel_path = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/1\"\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Available GPUs:\", torch.cuda.device_count())\nprint(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n\n# Load vLLM model (automatically uses GPU if available)\nllm = VLLM(model=model_path, dtype=\"half\", trust_remote_code=True, tensor_parallel_size=2, device=\"cuda\")\n\n\n# Define chat prompt\nchat_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI assistant. Answer user questions concisely and clearly.\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\n# Define chat pipeline using LangChain\nchat_chain = chat_prompt | llm\n\n# Chat loop\ndef start_chat():\n    print(\"Math & AI/ML/DL Assistant - Type 'exit' to quit\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n        response = chat_chain.invoke({\"input\": user_input})\n        print(f\"AI: {response}\")\n\n# Start chat session\nstart_chat()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}
