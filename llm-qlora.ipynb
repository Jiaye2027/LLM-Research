{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":11376393,"sourceType":"competition"},{"sourceId":238914,"sourceType":"modelInstanceVersion","modelInstanceId":204046,"modelId":225262},{"sourceId":238917,"sourceType":"modelInstanceVersion","modelInstanceId":204048,"modelId":225262},{"sourceId":238929,"sourceType":"modelInstanceVersion","modelInstanceId":204059,"modelId":225262}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''pip install torch transformers accelerate peft trl bitsandbytes datasets\npip uninstall -y torch torchvision torchaudio\npip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\npip install -U torch==2.3.1\npip install torchvision==0.17.2\npip install --no-cache-dir bitsandbytes'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install --no-cache-dir bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport torch\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import Trainer\nimport re\n\n# Load tokenizer and configure padding\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\ntokenizer.pad_token = tokenizer.eos_token  # Set pad token\n\n# Configure model with 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Define LoRA config targeting specific modules\nlora_config = LoraConfig(\n    r=4,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"v_proj\"]  # Target key attention projections\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\nmodel.enable_input_require_grads()\n\n# Load and format dataset\ndf = pd.read_parquet(\"hf://datasets/yxsllgz-uts-org/Math_Consistency-Probability-Llama-3.2-1B-Instruct-style1/data/math-00000-of-00001.parquet\")\ndf = df[['problem', 'm_solution', 'solution']]\n\ndef format_prompt(sample):\n    return {\"text\": f\"User: {sample['problem']}\\n\\nAssistant: {sample['m_solution']}\\n\\n{sample['solution']}\"}\n\ndataset = Dataset.from_list(df.apply(format_prompt, axis=1).to_list())\n\n\ndef remove_repeated_sentences(text):\n    \"\"\"Removes repeated sentences while preserving order.\"\"\"\n    sentences = re.split(r'(?<!\\\\)\\. |\\n+', text.strip())  # Split by periods or newlines\n    seen = set()\n    filtered_sentences = []\n\n    for sentence in sentences:\n        sentence = sentence.strip()\n        if sentence and sentence not in seen:\n            seen.add(sentence)\n            filtered_sentences.append(sentence)\n\n    return \". \".join(filtered_sentences)  # Join sentences back\n\ndef tokenize_function(examples):\n    \"\"\"Cleans up text by removing duplicate sentences and then tokenizes it.\"\"\"\n    cleaned_texts = [remove_repeated_sentences(text) for text in examples[\"text\"]]  # ✅ Handle batched input\n    return tokenizer(\n        cleaned_texts,  # Process batch\n        truncation=True,\n        max_length=1024,\n        return_special_tokens_mask=False\n    )\n\n# ✅ Apply tokenization and REMOVE TEXT COLUMN\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=dataset.column_names  # Remove original text column\n)\n\n# Dynamic padding via data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T04:45:35.580489Z","iopub.execute_input":"2025-03-12T04:45:35.580802Z","iopub.status.idle":"2025-03-12T04:46:08.365079Z","shell.execute_reply.started":"2025-03-12T04:45:35.580770Z","shell.execute_reply":"2025-03-12T04:46:08.364360Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d4c52838c845cc83becf9794cbdbdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8fa6783f304000b91c4547b6248e35"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Configure training arguments with 8-bit optimizer\ntraining_args = TrainingArguments(\n    output_dir=\"./qlora_deepseek\",\n    per_device_train_batch_size=2,          # Keep batch size at 1\n    gradient_accumulation_steps=4,          # Accumulate gradients\n    learning_rate=2e-4,\n    fp16=True,\n    optim=\"adamw_hf\",\n    #optim=\"adamw_bnb_8bit\",                 # Use memory-efficient optimizer\n    num_train_epochs=3,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False             # Important for custom data collation\n)\n\n# Initialize Trainer with data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,            # Dynamic padding\n    eval_dataset=tokenized_dataset.select(range(100))\n)\n\ntrainer.train()\n\nmodel.save_pretrained(\"qlora_deepseek-7B\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# runnable\nimport torch\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Set model path\nmodel_path = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Create a Hugging Face pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=2084,\n    temperature=0.7,\n    device_map=\"auto\"\n)\n\n# Wrap the pipeline in LangChain's HuggingFacePipeline\nllm = HuggingFacePipeline(pipeline=pipe)\n\n# Define the system prompt for math and AI/ML/DL\nsystem_prompt = \"\"\"You are an expert in mathematics and AI/ML/DL. You assist with:\n1. Mathematical concepts, proofs, and problem-solving.\n2. AI/ML/DL theory, algorithms, and applications.\n3. Code implementation for AI/ML/DL projects.\n4. Debugging and optimizing AI/ML/DL workflows.\n\nProvide clear, concise, and accurate responses. If the question is unclear, ask for clarification.\"\"\"\n\n# Set up the chat prompt template\nprompt_template = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_prompt),\n    HumanMessagePromptTemplate.from_template(\"{input}\")\n])\n\n# Use latest LangChain API (RunnableSequence)\nchat_chain = prompt_template | llm\n\n# Chat loop\ndef start_chat():\n    print(\"Math & AI/ML/DL Assistant - Type 'exit' to quit\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n        response = chat_chain.invoke({\"input\": user_input})\n        print(f\"AI: {response}\")\n\n# Start chat session\nstart_chat()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# pip install --target=/kaggle/working langchain_community\nimport os\nimport torch\nfrom langchain_community.llms import VLLM\nfrom langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom vllm import SamplingParams\n\n\n# Set environment variables (optional but recommended)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Adjust based on available GPUs\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid tokenizer parallelism conflicts\n\n\nmodel_path = \"/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1\"\n\n\n# Sampling Parameters for vLLM\nsampling_params = SamplingParams(\n    temperature=1.0,              # Control randomness\n    min_p=0.01,                   # Minimum cumulative probability for nucleus sampling\n    skip_special_tokens=True,      # Remove special tokens from output\n    max_tokens=8192,               # Maximum output length\n)\n\n# ✅ Use LangChain’s VLLM wrapper\nllm = VLLM(\n    model=model_path,\n    dtype=\"half\",                 # Use FP16 for efficiency\n    tensor_parallel_size=2,        # Use multiple GPUs (adjust as needed)\n    trust_remote_code=True,        # Trust remote execution for model and tokenizer\n    max_num_seqs=16,               # Maximum batch size per iteration\n    max_model_len=8192,            # Context length\n    gpu_memory_utilization=0.95,   # Use 95% of GPU memory\n    sampling_params=sampling_params # Pass the sampling params\n)\n\n# Define system prompt for AI/ML/DL assistance\nsystem_prompt = \"\"\"You are an expert in mathematics and AI/ML/DL. You assist with:\n1. Mathematical concepts, proofs, and problem-solving.\n2. AI/ML/DL theory, algorithms, and applications.\n3. Code implementation for AI/ML/DL projects.\n4. Debugging and optimizing AI/ML/DL workflows.\n\nProvide clear, concise, and accurate responses. If the question is unclear, ask for clarification.\"\"\"\n\n# Define chat prompt template\nprompt_template = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_prompt),\n    HumanMessagePromptTemplate.from_template(\"{input}\")\n])\n\n# ✅ Create LangChain Chat Pipeline with vLLM\nchat_chain = prompt_template | llm\n\n# Interactive Chat Loop\ndef start_chat():\n    print(\"Math & AI/ML/DL Assistant - Type 'exit' to quit\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n        response = chat_chain.invoke({\"input\": user_input})\n        print(f\"AI: {response}\")\n\n# Start Chat\nstart_chat()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_chat()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}