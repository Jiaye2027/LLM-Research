{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d918dc0",
   "metadata": {
    "papermill": {
     "duration": 0.005186,
     "end_time": "2025-04-21T06:16:06.040243",
     "exception": false,
     "start_time": "2025-04-21T06:16:06.035057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MeetingMate - A GenAI Powered Smart Meeting Companion\n",
    "\n",
    "Welcome to the capstone project of the Generative AI Intensive Course!  \n",
    "\n",
    "Problem: Tired of Chaotic, Unproductive Meetings?\n",
    "We’ve all been there—meetings that run long, feel unstructured, and leave you wondering what actually got decided. It’s hard to keep track of key takeaways, next steps, or even who said what. And when the meeting ends? You’re left digging through scattered notes or asking, “Wait, what did we agree on?”\n",
    "\n",
    "Solution: Meet Your Meeting’s New Best Friend - MeetingMate.\n",
    "\n",
    "Just upload your meeting audio, and MeetingMate takes care of the rest. It gives you a clean summary, a Q&A chatbot trained on your discussion, and even a downloadable slide deck. Built with OpenAI’s Whisper, Google Gemini, ChromaDB, and smart Python automation, MeetingMate turns your conversations into clear, actionable insights automatically.\n",
    "\n",
    "We combine various tools such as Whisper for transcription, the Gemini API for content generation, and PowerPoint automation to create presentation-ready summaries. This notebook also introduces semantic retrieval with ChromaDB for contextual grounding.\n",
    "\n",
    "---\n",
    "## Key Features Demonstrated\n",
    "\n",
    "- **Audio Understanding**: Transcribe meeting audio recordings using OpenAI's Whisper model.\n",
    "- **Document Understanding**: Generate structured slide presentations using `python-pptx`.\n",
    "- **Prompting**: Craft prompts for summarization, slide generation, and role-based insights.\n",
    "- **Structured Output / JSON Mode**: Utilize `google.generativeai.types` to format and parse model outputs.\n",
    "- **GenAI Evaluation**: Define rubric-based feedback using prompts to review generated outputs.\n",
    "- **Embeddings & Retrieval-Augmented Generation (RAG)**: Use ChromaDB to store and retrieve prior meeting data based on semantic similarity.\n",
    "\n",
    "---\n",
    "## What We Built\n",
    "\n",
    "MeetingMate takes in a meeting audio file and walks through the following pipeline:\n",
    "\n",
    "1. **Transcription**: Converts meeting audio to text via Whisper.\n",
    "2. **Summarization**: Uses the Gemini model to summarize the transcript.\n",
    "3. **Slide Generation**: Automatically creates a PowerPoint deck summarizing key takeaways.\n",
    "4. **Retrieval**: Retrieves relevant past meeting segments using ChromaDB and embeddings.\n",
    "5. **Evaluation**: Grades the output using structured GenAI-based rubrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- [Whisper](https://github.com/openai/whisper) for audio transcription\n",
    "- [Gemini API](https://ai.google.dev/) for text generation\n",
    "- [python-pptx](https://python-pptx.readthedocs.io/) for slide creation\n",
    "- [ChromaDB](https://docs.trychroma.com/) for semantic search\n",
    "\n",
    "---\n",
    "\n",
    "## Let’s Get Started!\n",
    "\n",
    "Follow the notebook cells sequentially to build and customize your MeetingMate assistant.\n",
    "Make sure your API keys are configured and all dependencies are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62799959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:16:06.050229Z",
     "iopub.status.busy": "2025-04-21T06:16:06.049549Z",
     "iopub.status.idle": "2025-04-21T06:18:05.704210Z",
     "shell.execute_reply": "2025-04-21T06:18:05.703251Z"
    },
    "papermill": {
     "duration": 119.661332,
     "end_time": "2025-04-21T06:18:05.705914",
     "exception": false,
     "start_time": "2025-04-21T06:16:06.044582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/openai/whisper.git python-pptx google-generativeai\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e02b2a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:05.763430Z",
     "iopub.status.busy": "2025-04-21T06:18:05.762545Z",
     "iopub.status.idle": "2025-04-21T06:18:12.869025Z",
     "shell.execute_reply": "2025-04-21T06:18:12.868126Z"
    },
    "papermill": {
     "duration": 7.137024,
     "end_time": "2025-04-21T06:18:12.870581",
     "exception": false,
     "start_time": "2025-04-21T06:18:05.733557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from google import genai\n",
    "\n",
    "import whisper\n",
    "from pptx import Presentation\n",
    "from pptx.util import Pt, Inches\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from google.genai import types, Client\n",
    "from google.api_core import retry\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import EmbeddingFunction\n",
    "import enum\n",
    "from IPython.display import Markdown, display\n",
    "from pptx.util import Inches\n",
    "from PIL import Image, ImageEnhance\n",
    "#from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8b571f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:12.928913Z",
     "iopub.status.busy": "2025-04-21T06:18:12.928133Z",
     "iopub.status.idle": "2025-04-21T06:18:12.932998Z",
     "shell.execute_reply": "2025-04-21T06:18:12.932156Z"
    },
    "papermill": {
     "duration": 0.035987,
     "end_time": "2025-04-21T06:18:12.934373",
     "exception": false,
     "start_time": "2025-04-21T06:18:12.898386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b618efb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:12.991468Z",
     "iopub.status.busy": "2025-04-21T06:18:12.990587Z",
     "iopub.status.idle": "2025-04-21T06:18:12.997628Z",
     "shell.execute_reply": "2025-04-21T06:18:12.996487Z"
    },
    "papermill": {
     "duration": 0.037247,
     "end_time": "2025-04-21T06:18:12.999157",
     "exception": false,
     "start_time": "2025-04-21T06:18:12.961910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Constants\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "CHROMA_STORAGE_PATH = Path(\"./chroma_storage\")\n",
    "COLLECTION_NAME = \"meeting_summary_collection\"\n",
    "\n",
    "PPTX_FILENAME = \"Meeting_Summary.pptx\"\n",
    "MEETING_THEMES = {\n",
    "    \"team_sync\": {\"title\": \"Team Sync\", \"color\": \"#2E86C1\", \"bg_image\": Path(\"/kaggle/input/meeting-images/meeting-2.jpg\")},\n",
    "    \"project_kickoff\": {\"title\": \"Project Kickoff\", \"color\": \"#27AE60\", \"bg_image\": Path(\"/kaggle/input/meeting-images/meeting-1.jpg\")},\n",
    "    \"retrospective\": {\"title\": \"Sprint Retrospective\", \"color\": \"#E67E22\", \"bg_image\": Path(\"/kaggle/input/meeting-images/meeting-4.jpg\")},\n",
    "    \"client_review\": {\"title\": \"Client Review\", \"color\": \"#8E44AD\", \"bg_image\": Path(\"/kaggle/input/meeting-images/meeting-3.jpg\")},\n",
    "    \"default\": {\"title\": \"Meeting Summary\", \"color\": \"#34495E\", \"bg_image\": Path(\"/kaggle/input/meeting-images/meeting-5.jpg\")},\n",
    "}\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 10000\n",
    "TEMPERATURE = 0.4\n",
    "TOP_P = 0.9\n",
    "TOP_K = 40\n",
    "EMBEDDING_TASK = \"retrieval_document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875b4f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.055902Z",
     "iopub.status.busy": "2025-04-21T06:18:13.055471Z",
     "iopub.status.idle": "2025-04-21T06:18:13.060446Z",
     "shell.execute_reply": "2025-04-21T06:18:13.059584Z"
    },
    "papermill": {
     "duration": 0.035477,
     "end_time": "2025-04-21T06:18:13.062011",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.026534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "# Suppress httpx and google_genai.models INFO logs by default\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"google_genai.models\").setLevel(logging.WARNING)\n",
    "\n",
    "# Configure root logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714dca8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.118741Z",
     "iopub.status.busy": "2025-04-21T06:18:13.118142Z",
     "iopub.status.idle": "2025-04-21T06:18:13.209371Z",
     "shell.execute_reply": "2025-04-21T06:18:13.208473Z"
    },
    "papermill": {
     "duration": 0.121525,
     "end_time": "2025-04-21T06:18:13.211096",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.089571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set api key\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b139e",
   "metadata": {
    "papermill": {
     "duration": 0.093379,
     "end_time": "2025-04-21T06:18:13.332831",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.239452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Upload Audio File\n",
    "\n",
    "This utility saves uploaded audio content to a temporary `.wav` file \n",
    "for processing by the transcription model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233a1e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.389391Z",
     "iopub.status.busy": "2025-04-21T06:18:13.389083Z",
     "iopub.status.idle": "2025-04-21T06:18:13.403983Z",
     "shell.execute_reply": "2025-04-21T06:18:13.403028Z"
    },
    "papermill": {
     "duration": 0.044895,
     "end_time": "2025-04-21T06:18:13.405367",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.360472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using audio file: /kaggle/input/audio-input/spkr0.wav\n"
     ]
    }
   ],
   "source": [
    "audio_dir = Path('/kaggle/input')\n",
    "wav_files = [f for f in audio_dir.rglob('*') if f.suffix in ['.wav', '.mp3']]\n",
    "if not wav_files:\n",
    "    raise FileNotFoundError(\"No .wav files found in /kaggle/input\")\n",
    "# Consider the first audio file\n",
    "src_path = wav_files[0]\n",
    "print(\"Using audio file:\", src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee2fcfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.463259Z",
     "iopub.status.busy": "2025-04-21T06:18:13.462514Z",
     "iopub.status.idle": "2025-04-21T06:18:13.466993Z",
     "shell.execute_reply": "2025-04-21T06:18:13.466228Z"
    },
    "papermill": {
     "duration": 0.034295,
     "end_time": "2025-04-21T06:18:13.468319",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.434024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save audio file\n",
    "def save_file(content):\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav') \n",
    "    temp_file.write(content)\n",
    "    temp_file.close()\n",
    "    return Path(temp_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6381389",
   "metadata": {
    "papermill": {
     "duration": 0.027082,
     "end_time": "2025-04-21T06:18:13.523315",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.496233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Audio Transcription (Whisper)\n",
    "\n",
    "This class loads a Whisper model and provides transcription functionality.  \n",
    "It is used to convert spoken meeting content into raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1548e5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.584517Z",
     "iopub.status.busy": "2025-04-21T06:18:13.583714Z",
     "iopub.status.idle": "2025-04-21T06:18:13.589720Z",
     "shell.execute_reply": "2025-04-21T06:18:13.588855Z"
    },
    "papermill": {
     "duration": 0.037333,
     "end_time": "2025-04-21T06:18:13.591046",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.553713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioTranscriber:\n",
    "    \"\"\"\n",
    "    Transcribe speech to text using Whisper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name = \"base\"):\n",
    "        try:\n",
    "            self.model = whisper.load_model(model_name)\n",
    "            logger.info(f\"Loaded Whisper model '{model_name}'\")\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to load Whisper model.\")\n",
    "            raise\n",
    "\n",
    "    def transcribe(self, audio_path):\n",
    "        \"\"\"\n",
    "        Transcribe the audio file and return the transcript.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.model.transcribe(str(audio_path))\n",
    "            logger.info(\"Transcription successful.\")\n",
    "            return result.get(\"text\", \"\")\n",
    "        except Exception:\n",
    "            logger.exception(f\"Transcription failed for {audio_path}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9a5d0",
   "metadata": {
    "papermill": {
     "duration": 0.027465,
     "end_time": "2025-04-21T06:18:13.647283",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.619818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Meeting Summarization (Gemini)\n",
    "\n",
    "This class summarizes meeting transcripts using Google's Gemini API.  \n",
    "It sends a structured prompt and expects the response in JSON format, which is then parsed for downstream use like slide generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ceeb675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.705259Z",
     "iopub.status.busy": "2025-04-21T06:18:13.704591Z",
     "iopub.status.idle": "2025-04-21T06:18:13.710696Z",
     "shell.execute_reply": "2025-04-21T06:18:13.709867Z"
    },
    "papermill": {
     "duration": 0.037193,
     "end_time": "2025-04-21T06:18:13.712104",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.674911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeetingSummarizer:\n",
    "    \"\"\"\n",
    "    Summarizes meeting transcripts via Google GenAI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def summarize(self, transcript, prompt):\n",
    "        \"\"\"\n",
    "        Generates a structured summary (sections with titles, summary, bullets).\n",
    "        \"\"\"\n",
    "    \n",
    "        config = types.GenerateContentConfig(\n",
    "            max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "        )\n",
    "        try:\n",
    "            resp = self.client.models.generate_content(\n",
    "                model=MODEL_NAME, config=config, contents=[prompt, transcript]\n",
    "            )\n",
    "            summary_text = resp.text\n",
    "            json_str = summary_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "            summary_slides = json.loads(json_str)\n",
    "            logger.info(\"Parsed summary to JSON.\")\n",
    "            return summary_slides, summary_text\n",
    "        except Exception:\n",
    "            logger.exception(\"Summarization error.\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db46f2e",
   "metadata": {
    "papermill": {
     "duration": 0.027804,
     "end_time": "2025-04-21T06:18:13.767512",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.739708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation with Gemini (Rubric-Based)\n",
    "\n",
    "Here, a GenAI-based evaluator scores the generated summary for its presentation quality.\n",
    "\n",
    "The model is prompted with an evaluation rubric based on:\n",
    "- Slide structure\n",
    "- Groundedness\n",
    "- Conciseness\n",
    "- Fluency\n",
    "\n",
    "This enables an automated review loop that mimics human feedback for quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2706625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.828207Z",
     "iopub.status.busy": "2025-04-21T06:18:13.827869Z",
     "iopub.status.idle": "2025-04-21T06:18:13.835264Z",
     "shell.execute_reply": "2025-04-21T06:18:13.834238Z"
    },
    "papermill": {
     "duration": 0.041339,
     "end_time": "2025-04-21T06:18:13.836656",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.795317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SummaryRating(enum.Enum):\n",
    "    VERY_GOOD = '5'\n",
    "    GOOD = '4'\n",
    "    OK = '3'\n",
    "    BAD = '2'\n",
    "    VERY_BAD = '1'\n",
    "    \n",
    "class SummaryEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates Summary generated from transcript via Google GenAI.\n",
    "    \"\"\"\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def eval(self, summary, transcript, summary_prompt):\n",
    "        \"\"\"\n",
    "        Evaluate the summary of the transcript and provide a rating on a scale 1 to 5, 1 being \"Very Poor\" and 5 being \"Excellent\"\n",
    "        \"\"\"\n",
    "        prompt = (f\"\"\"\n",
    "            # Instruction\n",
    "            You are an expert evaluator for slide presentations. Your task is to evaluate the quality of a meeting summary generated for a transcript by an AI model, which is intended to be turned into a PowerPoint slide deck.\n",
    "            \n",
    "            We will provide you with the original prompt and transcript given to the model and the AI-generated structured summary. Your evaluation should focus on whether this summary is effective for slide-based presentation.\n",
    "            \n",
    "            # Evaluation\n",
    "            ## Metric Definition\n",
    "            You will assess the meeting summary’s quality with regard to slide-readiness. A good slide summary should:\n",
    "            - Break the content into clear sections\n",
    "            - Contain accurate and concise summaries\n",
    "            - Use bullet points that can be used directly in presentation slides\n",
    "            - Avoid introducing information that wasn't in the source\n",
    "            \n",
    "            ## Criteria\n",
    "            1. **Structure for Slides**: The summary is clearly broken down into presentation-friendly sections with meaningful titles.\n",
    "            2. **Groundedness**: The summary uses only content grounded in the original meeting transcript and does not hallucinate.\n",
    "            3. **Conciseness and Slide-Readiness**: The bullets are clear, well-chunked, and ready to be used on slides (not full paragraphs).\n",
    "            4. **Fluency and Readability**: The summaries and bullets are easy to understand and grammatically correct.\n",
    "            \n",
    "            ## Rating Rubric\n",
    "            5 (Excellent): Summary is well-structured, fully grounded, concise, and presentation-ready with fluent writing.\n",
    "            4 (Good): Summary is mostly well-structured and grounded; bullets are usable with minor edits.\n",
    "            3 (Fair): Summary is okay but needs editing to be usable in slides (e.g., too verbose, not well-structured).\n",
    "            2 (Poor): Summary is grounded but hard to use in a slide deck without major revisions.\n",
    "            1 (Very Poor): Summary is ungrounded, off-topic, or incoherent.\n",
    "            \n",
    "            ## Evaluation Steps\n",
    "            STEP 1: Assess the summary for presentation-readiness using the 4 criteria.\n",
    "            STEP 2: Score the summary using the rubric.\n",
    "            \n",
    "            # User Inputs and AI-generated Response\n",
    "            ## Prompt\n",
    "            {summary_prompt}\n",
    "            ## transcript\n",
    "            {transcript}\n",
    "            \n",
    "            ## AI-generated Summary (JSON format intended for slide generation)\n",
    "            ```\n",
    "            {summary}\n",
    "            ```\n",
    "                \"\"\"\n",
    "        )\n",
    "        try:\n",
    "            resp = self.client.chats.create(\n",
    "                model=MODEL_NAME).send_message(prompt)\n",
    "            verbose_eval = resp.text\n",
    "            logger.info(\"Evaluated the summary generated.\")\n",
    "            return verbose_eval\n",
    "        except Exception:\n",
    "            logger.exception(\"Evaluation error.\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d99ad4",
   "metadata": {
    "papermill": {
     "duration": 0.027438,
     "end_time": "2025-04-21T06:18:13.891870",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.864432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Slide Generation (python-pptx)\n",
    "\n",
    "This section uses the `python-pptx` library to automatically convert the structured JSON summary \n",
    "into a clean and readable PowerPoint presentation. It includes:\n",
    "- Section titles\n",
    "- Summaries\n",
    "- Bullet points styled for readability\n",
    "\n",
    "The goal is to reduce manual effort in summarizing and preparing meeting notes into presentable slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2febff2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:13.948564Z",
     "iopub.status.busy": "2025-04-21T06:18:13.948220Z",
     "iopub.status.idle": "2025-04-21T06:18:13.964742Z",
     "shell.execute_reply": "2025-04-21T06:18:13.964055Z"
    },
    "papermill": {
     "duration": 0.046876,
     "end_time": "2025-04-21T06:18:13.966202",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.919326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPTGenerator:\n",
    "    \"\"\"\n",
    "    Create a PowerPoint from structured summary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, themes):\n",
    "        self.themes = themes\n",
    "\n",
    "    @staticmethod\n",
    "    def hex_to_rgb(h):\n",
    "        h = h.lstrip(\"#\")\n",
    "        return tuple(int(h[i : i + 2], 16) for i in (0, 2, 4))\n",
    "\n",
    "    @staticmethod\n",
    "    def is_dark(rgb):\n",
    "        r, g, b = rgb\n",
    "        return (0.299 * r + 0.587 * g + 0.114 * b) < 150\n",
    "\n",
    "    @staticmethod\n",
    "    def add_bg(slide, rgb, image_path):\n",
    "        \"\"\"\n",
    "        Adds a background image or fallback color to the slide.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if image_path:\n",
    "                # Resize image first\n",
    "                with Image.open(image_path) as img:\n",
    "                    img = img.resize((960, 720))  # PowerPoint slide size\n",
    "                    temp_path = \"/tmp/resized_bg.jpg\"\n",
    "                    img.save(temp_path)\n",
    "    \n",
    "                # Apply opacity to resized image\n",
    "                faded_path = \"/tmp/faded_bg.png\"\n",
    "                PPTGenerator.apply_opacity(temp_path, faded_path, opacity=0.3)\n",
    "    \n",
    "                # Add faded image to slide\n",
    "                img_shape = slide.shapes.add_picture(faded_path, 0, 0, width=Inches(10), height=Inches(7.5))\n",
    "            \n",
    "                # Send to back of z-order\n",
    "                spTree = slide.shapes._spTree\n",
    "                spTree.remove(img_shape._element)\n",
    "                spTree.insert(2, img_shape._element)\n",
    "                return\n",
    "        except Exception:\n",
    "            logger.exception(\"Background image failed. Falling back to color.\")\n",
    "    \n",
    "        # Fallback to solid color background\n",
    "        if rgb:\n",
    "            shape = slide.shapes.add_shape(\n",
    "                MSO_SHAPE.RECTANGLE, Inches(0), Inches(0), Inches(10), Inches(7.5)\n",
    "            )\n",
    "            shape.fill.solid()\n",
    "            shape.fill.fore_color.rgb = RGBColor(*rgb)\n",
    "            shape.line.fill.background()\n",
    "            slide.shapes._spTree.insert(2, slide.shapes._spTree[-1])\n",
    "    @staticmethod\n",
    "    def apply_opacity(image_path, output_path, opacity = 0.3):\n",
    "        \"\"\"\n",
    "        Saves a faded version of the image to use as background.\n",
    "        \"\"\"\n",
    "        img = Image.open(image_path).convert(\"RGBA\")\n",
    "        alpha = img.split()[3]\n",
    "        alpha = ImageEnhance.Brightness(alpha).enhance(opacity)\n",
    "        img.putalpha(alpha)\n",
    "        img.save(output_path)\n",
    "\n",
    "    def generate(self, sections, filename, mtype = \"default\"):\n",
    "        \"\"\"\n",
    "        Build and save the .pptx file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            theme = MEETING_THEMES.get(mtype, MEETING_THEMES[\"default\"])\n",
    "            rgb = self.hex_to_rgb(theme[\"color\"])\n",
    "            bg_image = theme.get(\"bg_image\")\n",
    "            font_rgb = (0, 0, 0)\n",
    "            use_light_text = self.is_dark(rgb)\n",
    "            prs = Presentation()\n",
    "\n",
    "            # Title Slide\n",
    "            slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
    "            self.add_bg(slide, rgb=rgb, image_path=bg_image)\n",
    "            slide.shapes.title.text = theme[\"title\"]\n",
    "            slide.shapes.title.text_frame.paragraphs[0].font.color.rgb = RGBColor(\n",
    "                *font_rgb\n",
    "            )\n",
    "            body = slide.placeholders[1]\n",
    "            body.text = filename.stem\n",
    "            body.text_frame.paragraphs[0].font.color.rgb = RGBColor(*font_rgb)\n",
    "\n",
    "            # TOC\n",
    "            toc = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "            self.add_bg(toc, rgb=rgb, image_path=bg_image)\n",
    "            toc.shapes.title.text = \"Table of Contents\"\n",
    "            toc.shapes.title.text_frame.paragraphs[0].font.color.rgb = RGBColor(\n",
    "                *font_rgb\n",
    "            )\n",
    "            toc_body = toc.placeholders[1]\n",
    "            toc_body.text = \"\\n\".join(s[\"section_title\"] for s in sections)\n",
    "            for p in toc_body.text_frame.paragraphs:\n",
    "                p.font.color.rgb = RGBColor(*font_rgb)\n",
    "\n",
    "            # Content Slides\n",
    "            for sec in sections:\n",
    "                sld = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "                self.add_bg(sld, rgb=rgb, image_path=bg_image)\n",
    "                sld.shapes.title.text = sec[\"section_title\"]\n",
    "                sld.shapes.title.text_frame.paragraphs[0].font.color.rgb = RGBColor(\n",
    "                    *font_rgb\n",
    "                )\n",
    "                box = sld.placeholders[1]\n",
    "                tf = box.text_frame\n",
    "                tf.clear()\n",
    "                summary_text = sec.get(\"summary\", \"\")\n",
    "                summary_pt = tf.add_paragraph()\n",
    "                summary_pt.text = f\"Summary: {summary_text}\"\n",
    "                summary_pt.font.size = Pt(18)\n",
    "                summary_pt.font.color.rgb = RGBColor(*font_rgb)\n",
    "                summary_pt.font.bold = True\n",
    "                for bullet in sec.get(\"bullets\", []):\n",
    "                    pb = tf.add_paragraph()\n",
    "                    pb.text = bullet\n",
    "                    pb.level = 1\n",
    "                    pb.font.size = Pt(20)\n",
    "                    pb.font.color.rgb = RGBColor(*font_rgb)\n",
    "                    \n",
    "\n",
    "            prs.save(str(filename))\n",
    "            logger.info(f\"PPT saved: {filename}\")\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed PPT generation.\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5280b0c",
   "metadata": {
    "papermill": {
     "duration": 0.027306,
     "end_time": "2025-04-21T06:18:14.021480",
     "exception": false,
     "start_time": "2025-04-21T06:18:13.994174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embedding and Retrieval\n",
    "\n",
    "This part introduces **retrieval-augmented generation (RAG)** by embedding previous meeting transcripts \n",
    "and storing them in a vector database (`ChromaDB`). When a new meeting is processed, similar \n",
    "past meetings can be retrieved to provide context or track recurring themes.\n",
    "\n",
    "This enhances the agent’s memory and understanding of long-term patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64aa7cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:14.079881Z",
     "iopub.status.busy": "2025-04-21T06:18:14.079493Z",
     "iopub.status.idle": "2025-04-21T06:18:14.091339Z",
     "shell.execute_reply": "2025-04-21T06:18:14.090561Z"
    },
    "papermill": {
     "duration": 0.04294,
     "end_time": "2025-04-21T06:18:14.092693",
     "exception": false,
     "start_time": "2025-04-21T06:18:14.049753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAGEngine:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation using ChromaDB.\n",
    "    \"\"\"\n",
    "    def __init__(self, client, storage_path, collection_name):\n",
    "        self.client = client\n",
    "        self.storage_path = storage_path\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    class _EmbeddingFn(EmbeddingFunction):\n",
    "        def __init__(self, client, model):\n",
    "            self.client = client\n",
    "            self.model = model\n",
    "\n",
    "        def __call__(self, texts):\n",
    "            try:\n",
    "                res = self.client.models.embed_content(\n",
    "                    model=self.model,\n",
    "                    contents=texts,\n",
    "                    config=types.EmbedContentConfig(\n",
    "                    task_type=EMBEDDING_TASK,\n",
    "                    ),\n",
    "                )\n",
    "                raw = getattr(res, 'embeddings', None) or res.get('embeddings') or res.get('embedding')\n",
    "                if not raw:\n",
    "                    raise ValueError(\"No embeddings returned from GenAI.\")\n",
    "                processed = [item.values if hasattr(item, 'values') else item for item in raw]\n",
    "                return processed\n",
    "            except Exception:\n",
    "                logger.exception(\"Embedding failed.\")\n",
    "                return []\n",
    "\n",
    "    def init_db(self) :\n",
    "        \"\"\"\n",
    "        Initialize or get a ChromaDB collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            client = chromadb.PersistentClient(path=str(self.storage_path))\n",
    "            existing = [col.name for col in client.list_collections()]\n",
    "            if self.collection_name in existing:\n",
    "                logger.info(\"Using existing ChromaDB collection.\")\n",
    "                return client.get_collection(name=self.collection_name)\n",
    "            logger.info(f\"Creating new ChromaDB collection: {self.collection_name}\")\n",
    "            return client.create_collection(\n",
    "                name=self.collection_name,\n",
    "                embedding_function=self._EmbeddingFn(self.client, EMBEDDING_MODEL)\n",
    "            )\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to initialize ChromaDB collection.\")\n",
    "            raise\n",
    "\n",
    "    def add_document(self, db, text):\n",
    "        \"\"\"\n",
    "        Add a document to ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if text:\n",
    "                db.add(documents=[text], ids=[str(uuid.uuid4())])\n",
    "                logger.info(\"Document added to ChromaDB.\")\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to add document to ChromaDB.\")\n",
    "\n",
    "    def query(self, db, query, k = 2):\n",
    "        \"\"\"\n",
    "        Query ChromaDB for top-k relevant documents.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = db.query(query_texts=[query], n_results=k)\n",
    "            return result.get('documents', [])\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to query ChromaDB.\")\n",
    "            return []\n",
    "\n",
    "    def answer(self, db, query, k = 2):\n",
    "        \"\"\"\n",
    "        Answer a question using retrieved passages from ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            passages = self.query(db, query, k)\n",
    "            prompt = (\n",
    "                f\"\"\"You are a helpful and informative bot that answers questions using only the\n",
    "                provided reference passage below.\n",
    "                \n",
    "                Instructions:\n",
    "                - Provide a complete, well-explained answer based solely on the passage.\n",
    "                - If the answer is not available, respond with: \"I'm not sure.\"\n",
    "                - You are responding to a technical audience, so explain clearly but concisely.\n",
    "                - Break down complex concepts into understandable parts.\n",
    "                - Ignore irrelevant information.\n",
    "                \n",
    "                Passage:\n",
    "                {passages}\n",
    "                \n",
    "                Question:\n",
    "                {query}\n",
    "\n",
    "                Answer:RAG Q&A\"\"\"\n",
    "            )\n",
    "            resp = self.client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            return resp.text.strip()\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to generate answer.\")\n",
    "            return \"I'm not sure.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152634a",
   "metadata": {
    "papermill": {
     "duration": 0.027965,
     "end_time": "2025-04-21T06:18:14.148340",
     "exception": false,
     "start_time": "2025-04-21T06:18:14.120375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## End-to-End Workflow: Transcribe → Summarize → Evaluate → Generate Slides → Retrieve → QA\n",
    "\n",
    "This function orchestrates the full **MeetingMate pipeline**:\n",
    "\n",
    "1. **Transcribe**: Converts raw audio to text using Whisper.\n",
    "2. **Summarize**: Uses Gemini API with structured prompting to generate a JSON summary.\n",
    "3. **Evaluate**: Scores the summary using a rubric-based GenAI evaluator.\n",
    "4. **Detect Theme**: Classifies the meeting type (e.g., retrospective, kickoff).\n",
    "5. **Generate Slides**: Converts the summary into a PowerPoint presentation.\n",
    "6. **RAG Q&A**: Embeds the transcript and performs retrieval-augmented question answering using ChromaDB.\n",
    "\n",
    "This design demonstrates a complete GenAI application combining multimodal processing, structured prompting, evaluation, generation, and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34620464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T06:18:14.205970Z",
     "iopub.status.busy": "2025-04-21T06:18:14.205580Z",
     "iopub.status.idle": "2025-04-21T06:19:14.844984Z",
     "shell.execute_reply": "2025-04-21T06:19:14.844223Z"
    },
    "papermill": {
     "duration": 60.670062,
     "end_time": "2025-04-21T06:19:14.846536",
     "exception": false,
     "start_time": "2025-04-21T06:18:14.176474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 194MiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### **1. Structured output of the Summary**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "[\n",
       "  {\n",
       "    \"section_title\": \"Logistics of In-Person Meetings and Participant Incentives\",\n",
       "    \"summary\": \"The team discusses strategies to encourage attendance at in-person meetings, including offering incentives like transcripts and CDs of participants' speech, while also addressing concerns about data privacy and resource limitations.\",\n",
       "    \"bullets\": [\n",
       "      \"The team explores the idea of providing participants with a CD of their speech as an incentive, but raises concerns about privacy and data usage.\",\n",
       "      \"Offering transcripts after a screening phase is suggested as a compromise to address privacy concerns while still providing value to participants.\",\n",
       "      \"The team emphasizes the importance of recruiting a diverse group of participants, including those outside of linguistics and engineering.\"\n",
       "    ]\n",
       "  },\n",
       "  {\n",
       "    \"section_title\": \"Disk Space Management and Archiving Progress\",\n",
       "    \"summary\": \"The team provides an update on the progress of archiving files to free up disk space for recording future meetings, highlighting the time-consuming nature of the process and the benefits of having archived data readily accessible.\",\n",
       "    \"bullets\": [\n",
       "      \"Archiving old files, particularly from broadcast news, is underway to create more disk space for recording meetings.\",\n",
       "      \"The archiving process is time-intensive, but it allows for quick retrieval of data when needed.\",\n",
       "      \"After the current archiving is complete, the team will have enough space to record approximately five more meetings.\"\n",
       "    ]\n",
       "  }\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 2. Rubric-Based Evaluation\n",
       "## Evaluation\n",
       "Here's an evaluation of the AI-generated summary based on the criteria:\n",
       "\n",
       "**STEP 1: Assessment**\n",
       "\n",
       "*   **Structure for Slides**: The summary is well-structured, dividing the content into two distinct sections with clear and meaningful titles.\n",
       "*   **Groundedness**: The summary is grounded in the original meeting transcript. All points discussed are present in the source material.\n",
       "*   **Conciseness and Slide-Readiness**: The bullet points are concise and suitable for slides. They effectively summarize key points from the transcript.\n",
       "*   **Fluency and Readability**: The summaries and bullet points are easy to understand and grammatically correct.\n",
       "\n",
       "**STEP 2: Score**\n",
       "\n",
       "Based on the assessment, the summary is rated as **5 (Excellent)**. It is well-structured, fully grounded, concise, presentation-ready, and fluently written.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### **3. Meeting Type**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "team_sync"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### **4. RAG Q&A**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q:What's the main priority for getting people to the underused room?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Ans:The first priority should be to try to get people to come to the underused room because they are already set up for it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q:What audience groups are excluded?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Ans:The audience groups that are excluded are linguists and engineers because a wider sampling of people is needed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q:What's the plan for distributing audio recordings or 'CDs' post-session?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Ans:Initially, there was discussion about providing participants with a CD of their speech immediately after the session. However, concerns were raised regarding the content of these recordings and whether they were suitable for immediate release. The revised plan involves a two-step process: first, transcripts of the recordings will be screened. After this screening phase, participants will receive a CD. It has been specified that the CD given to the participants should be the same version that has been reviewed publicly to ensure compliance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### **Done**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(audio_bytes):\n",
    "    \"\"\"\n",
    "    Workflow: transcribe -> summarize and RAG Q&A -> detect type -> ppt\n",
    "    \"\"\"\n",
    "    try:\n",
    "        key = GOOGLE_API_KEY\n",
    "        gen_client = Client(api_key=key)\n",
    "\n",
    "        #### Transcription ####\n",
    "        transcriber = AudioTranscriber()\n",
    "        path = save_file(audio_bytes)\n",
    "        transcript = transcriber.transcribe(path)\n",
    "\n",
    "        #### Summarization ####\n",
    "        summary_prompt = (\"\"\"\n",
    "            You are a meeting assistant. Carefully analyze the meeting transcript and:\n",
    "                1. Segment it into distinct topics (whenever the conversation focus shifts).\n",
    "                2. For each topic:\n",
    "                  - Assign a meaningful short section_title (max 1 line).\n",
    "                  - Write a concise 1-sentence summary.\n",
    "                  - Extract 2-3 bullet points (each bullet under 50 words).\n",
    "                \n",
    "            Think step by step. Identify topic shifts chronologically.\n",
    "            Return the result strictly in the JSON format. All keys must be in double quotes:\n",
    "                ```\n",
    "                  {\n",
    "                    \"section_title\": \"Team Updates\",\n",
    "                    \"summary\": \"...\",\n",
    "                    \"bullets\": [\"...\", \"...\", \"...\"]\n",
    "                  },\n",
    "                  ...\n",
    "                ```\n",
    "                \"\"\"\n",
    "        )\n",
    "        summarizer = MeetingSummarizer(gen_client)\n",
    "        summary, summary_text = summarizer.summarize(transcript, summary_prompt)\n",
    "        display(Markdown(\"### **1. Structured output of the Summary**\"))\n",
    "        display(Markdown(summary_text))\n",
    "\n",
    "        #### Summary Evaluation ####\n",
    "        evaluate = SummaryEvaluator(gen_client)\n",
    "        evaluation = evaluate.eval(summary, transcript, summary_prompt)\n",
    "        display(Markdown(f\"### 2. Rubric-Based Evaluation\\n{evaluation}\"))\n",
    "        \n",
    "        #### Theme Detection ####\n",
    "        theme_prompt = (\"\"\"\n",
    "        You're a Meeting assistant. Given the context, understand it and choose the meeting type accordingly from the list:\n",
    "        [\"team_sync\", \"project_kickoff\", \"client_review\", \"retrospective\"].\n",
    "                \n",
    "        Reply with just the meeting type.\n",
    "        \"\"\"\n",
    "        )\n",
    "        theme_resp = gen_client.models.generate_content(\n",
    "            model=MODEL_NAME, contents=[theme_prompt, transcript]\n",
    "        )\n",
    "        mtype = theme_resp.text.strip().lower()\n",
    "        display(Markdown(\"### **3. Meeting Type**\"))\n",
    "        display(Markdown(mtype))\n",
    "\n",
    "        #### PPT generation ####\n",
    "        ppt = PPTGenerator(MEETING_THEMES)\n",
    "        ppt.generate(summary, Path(PPTX_FILENAME), mtype)\n",
    "\n",
    "        #### RAG Q&A using Chroma ####\n",
    "        rag = RAGEngine(gen_client, CHROMA_STORAGE_PATH, COLLECTION_NAME)\n",
    "        db = rag.init_db()\n",
    "        rag.add_document(db, transcript)\n",
    "\n",
    "\n",
    "        # Example QA\n",
    "        questions = [\n",
    "            \"What's the main priority for getting people to the underused room?\",\n",
    "            \"What audience groups are excluded?\",\n",
    "            \"What's the plan for distributing audio recordings or 'CDs' post-session?\",\n",
    "        ]\n",
    "\n",
    "        display(Markdown(\"### **4. RAG Q&A**\"))\n",
    "        for question in questions:\n",
    "            display(Markdown(\"Q:\" + question))\n",
    "            ans = rag.answer(db, question, k=2)\n",
    "            display(Markdown(\"Ans:\"+ans))\n",
    "\n",
    "    except Exception:\n",
    "        logger.exception(\"Main workflow error.\")\n",
    "        raise\n",
    "    display(Markdown(\"### **Done**\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_bytes = src_path.read_bytes()\n",
    "    main(audio_bytes)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    },
    {
     "datasetId": 7195486,
     "sourceId": 11480462,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7197805,
     "sourceId": 11484150,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 196.182029,
   "end_time": "2025-04-21T06:19:17.729987",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-21T06:16:01.547958",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
